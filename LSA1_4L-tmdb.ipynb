{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as nm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "2400 2400\n",
      "1600 1600\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#  Load the raw text dataset.\n",
    "###############################################################################\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# The raw text dataset is stored as tuple in the form:\n",
    "# (X_train_raw, y_train_raw, X_test_raw, y_test)\n",
    "# The 'filtered' dataset excludes any articles that we failed to retrieve\n",
    "# fingerprints for.\n",
    "'''categories = ['comp.graphics', 'sci.space','rec.sport.baseball','sci.electronics']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',categories=categories)\n",
    "\n",
    "X_train_raw = newsgroups_train.data\n",
    "#y_train_labels = newsgroups_train.target\n",
    "X_test_raw = newsgroups_test.data\n",
    "#y_test_labels = newsgroups_test.target\n",
    "\n",
    "print(len(X_train_raw))\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target'''\n",
    "\n",
    "categories = ['action_adventure','drama_romance','comedy_family','horror_thriller']\n",
    "tmdbmovies_train = pd.read_csv(\"total_train.csv\")\n",
    "tmdbmovies_test = pd.read_csv(\"total_test.csv\")\n",
    "X_train_raw = tmdbmovies_train[\"data\"]\n",
    "y_train = tmdbmovies_train[\"Label\"]\n",
    "X_test_raw = tmdbmovies_test[\"data\"]\n",
    "y_test = tmdbmovies_test[\"Label\"]\n",
    "print(len(X_train_raw),len(y_train))\n",
    "print(len(X_test_raw),len(y_test))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "lda = LatentDirichletAllocation()\n",
    "# Build the tfidf vectorizer from the training data (\"fit\"), and apply it \n",
    "# (\"transform\").\n",
    "vectorizer.fit(X_train_raw)\n",
    "X_train_tfidf = vectorizer.transform(X_train_raw).toarray()\n",
    "print(\"done\")\n",
    "#print(\"  Actual number of tfidf features: %d\" % X_train_tfidf.get_shape()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing dimensionality reduction using LSA\n",
      "  done in 2.433sec\n",
      "  Explained variance of the SVD step: 16%\n",
      "\n",
      "Classifying LSA vectors...\n",
      "  (599 / 1600) correct - 37.44%\n",
      "    done in 0.216sec\n",
      "========================TFIDF + LSA + Cosine======================\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "action_adventure       0.39      0.59      0.47       453\n",
      "   drama_romance       0.36      0.46      0.40       446\n",
      "   comedy_family       0.38      0.23      0.29       409\n",
      " horror_thriller       0.36      0.11      0.17       292\n",
      "\n",
      "        accuracy                           0.37      1600\n",
      "       macro avg       0.37      0.35      0.33      1600\n",
      "    weighted avg       0.37      0.37      0.35      1600\n",
      "\n",
      "confusion matrix:\n",
      "                          comp.graphics:pred  sci.space:pred  \\\n",
      "comp.graphics:true                      268             104   \n",
      "sci.space:true                          162             204   \n",
      "rec.sport.baseball:true                 144             155   \n",
      "sci.electronics:true                    119             105   \n",
      "All                                     693             568   \n",
      "\n",
      "                         rec.sport.baseball:pred  sci.electronics:pred   All  \n",
      "comp.graphics:true                            62                    19   453  \n",
      "sci.space:true                                58                    22   446  \n",
      "rec.sport.baseball:true                       95                    15   409  \n",
      "sci.electronics:true                          36                    32   292  \n",
      "All                                          251                    88  1600  \n",
      "accuracy: 37.4375\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerforming dimensionality reduction using LSA\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Project the tfidf vectors onto the first N principal components.\n",
    "# Though this is significantly fewer features than the original tfidf vector,\n",
    "# they are stronger features, and the accuracy is higher.\n",
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "print(\"  done in %.3fsec\" % (time.time() - t0))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "\n",
    "\n",
    "# Now apply the transformations to the test data as well.\n",
    "X_test_tfidf = vectorizer.transform(X_test_raw).toarray()\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#  Run classification of the test articles\n",
    "###############################################################################\n",
    "\n",
    "print(\"\\nClassifying LSA vectors...\")\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "# Build a k-NN classifier. Use k = 5 (majority wins), the cosine distance, \n",
    "# and brute-force calculation of distances.\n",
    "knn_lsa = KNeighborsClassifier(n_neighbors=5,metric='cosine')\n",
    "knn_lsa.fit(X_train_lsa, y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_lsa.predict(X_test_lsa)\n",
    "\n",
    "\n",
    "\n",
    "# Measure accuracy\n",
    "numRight = 0;\n",
    "for i in range(0,len(p)):\n",
    "    if p[i] == y_test[i]:\n",
    "        numRight += 1\n",
    "\n",
    "print(\"  (%d / %d) correct - %.2f%%\" % (numRight, len(y_test), float(numRight) / float(len(y_test)) * 100.0))\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)    \n",
    "print(\"    done in %.3fsec\" % elapsed)\n",
    "\n",
    "y_pred = p\n",
    "print(\"========================TFIDF + LSA + Cosine======================\")\n",
    "print(classification_report(y_test, y_pred, target_names=categories))\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, y_pred),index=['comp.graphics:true', 'sci.space:true','rec.sport.baseball:true','sci.electronics:true'],columns=['comp.graphics:pred', 'sci.space:pred','rec.sport.baseball:pred','sci.electronics:pred'])\n",
    "#cm = pd.DataFrame(pd.crosstab(y_test,y_pred),index=categories,columns=categories)\n",
    "cw = cm.sum(axis=0)\n",
    "row_df = pd.DataFrame([cw],index=[\"All\"])\n",
    "cm = pd.concat([ cm,row_df])\n",
    "cm[\"All\"] = cm.sum(axis=1)\n",
    "print(\"confusion matrix:\\n\",cm)\n",
    "print(\"accuracy:\",metrics.accuracy_score(y_test, y_pred)*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitdf6fafea6f864b21882d7b87f917e4eb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
